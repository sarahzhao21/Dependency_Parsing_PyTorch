# Dependency_Parsing_by_PyTorch
Despite its seeming chaos, natural language has lots of structure. Weâ€™ve already seen some of this structure in part of speech tags and how the order of parts of speech are predictive of what kinds of words might come next (via their parts of speech).

In this project, I have got a deeper view of this structure by implementing a dependency parser. Dependency parsing identifies the syntactic relationship between word pairs to create a parse tree. I implemented the shift-reduce neural dependency parser of Chen and Manning [2014], which was one of the first neural network-based parser and is quite famous. And PyTorcfh is the library that has been used in this work. This project include 1)finish the implementation of the neural network and 2) fill in the main training loop that processes each batch of instances and does backprop and 3)Test the performance of the parser. 
[For details and python codes, please look at this file.](https://github.com/sarahzhao21/Dependency_Parsing_PyTorch/blob/f47e6e9cb3522e9938da148e8d3579ed14be9183/main.py)

There are two slightly different models in this study: [model1](https://github.com/sarahzhao21/Dependency_Parsing_PyTorch/blob/8994369435e3964d70e056e4bc63edeec778c699/model.py) and [model2](https://github.com/sarahzhao21/Dependency_Parsing_PyTorch/blob/8994369435e3964d70e056e4bc63edeec778c699/model2.py). Because I add more dense layers in model2, that makes the performance of model2 better than model1.   
![Accuracy of the two models](https://github.com/sarahzhao21/Dependency_Parsing_PyTorch/blob/8994369435e3964d70e056e4bc63edeec778c699/acc.png)![Loss of the two models](https://github.com/sarahzhao21/Dependency_Parsing_PyTorch/blob/8994369435e3964d70e056e4bc63edeec778c699/loss.png)
